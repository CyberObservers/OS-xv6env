# xv6实验报告


# Lab00 Tools 环境搭建

## 激活WSL服务

控制面板->启动或关闭Windows功能->选择"Windows虚拟机监控程序平台", "适用于Linux的Windows子系统"和"虚拟机平台", 接着重启电脑.

## 安装操作系统

在终端输入以下指令(以Ubuntu 20.04为例):

```
wsl --install -d Ubuntu-20.04
```

## VS code远程连接Linux

安装Remote-WSL插件, 然后在Linux目录下输入: 

```
$ code .
```

即可自动连接VS code, 也可以通过VS code连接

## xv6环境配置: 安装RISC-V等工具

在Linux下输入以下命令即可

```bash
$ sudo apt-get update && sudo apt-get upgrade
$ sudo apt-get install git build-essential gdb-multiarch qemu-system-misc gcc-riscv64-linux-gnu binutils-riscv64-linux-gnu
```

测试安装:

```bash
# in the xv6 directory
$ make qemu
# ... lots of output ...
init: starting sh
$
```

## 克隆实验仓库

若无git, 请先安装git. 执行以下代码: 

```bash
$ git clone git://g.csail.mit.edu/xv6-labs-2021
```

## 配置远程仓库

非本实验重点, 仅提及.

1.  在GitHub上创建一个仓库
2.  在WSL中填写邮箱与姓名信息
3.  在WSL中生成并获取ssh密钥
4.  在GitHub中添加ssh密钥
5.  复制仓库的SSH key
6.  在本地仓库目录下更改远程仓库url
7.  将本地分支push到远程仓库
    常用指令:

```bash
$ git add . # 将修改添加到暂存区
$ git commit -m "msg" # 保存所有本地修改, 并添加注解(msg)
$ git push origin branch_name # 将某一分支push到远程仓库
$ git remote -v # 查看push和pull的远程仓库名
$ git status # 查看状态
```

**配置完成!**

## 实验中的git命令

若要切换到其他分支且本分支有修改，则需要：

```bash
$ git add .	# 将修改添加到文件树
$ git commit -m "msg" # 确认修改，并注解
$ git push # 推送到远程仓库
$ git checkout <branch_name> # 切换分支
```

**若不保存切换不了分支，若切换了则不会保存修改！！！**

# Lab01 Utility

## Boot xv6

#### 1. 实验目的

启动xv6操作系统

#### 2. 实验内容

输入命令:

```bash
$ make qemu
```

#### 3. 遇到的问题及解决方法

无

#### 4. 实验心得

成功启动xv6.


## sleep

#### 1. 实验目的

通过编写UNIX程序实现系统``sleep``功能. ``sleep``接收一个整型参数: 休眠的时间.编写的程序应为``user/sleep.c``

#### 2. 实验内容

1.  阅读``user/``目录下的其他程序(如``user/echo.c``, ``user/grep.c``等)了解如何获取命令行参数.
2.  新建``user/sleep.c``, 编写程序.用``int main(int argc, char **argv)``传参.``argv[0]``是可执行文件名; ``argc``是参数个数, 最小为1, 即argv[0]. 若``argc==1``则告知用户需要输入时间.若参数合法则调用系统调用sleep.最后退出程序.
3.  更改``Makefile``中的``UPROGS``，加入``$U/_sleep\``

#### 3. 遇到的问题及解决方法

在xv6的用户态下程序不能调用``return 0``而应该用``exit(0)``.

#### 4. 实验心得

这个实验是xv6实验的热身, 体验了从用户态通过系统调用访问内核态.

#### 5. 实验截图

![sleep截图](E:/大二下/操作系统/课设文档/src/Lab01/sleep.bmp "sleep实验截图")

## pingpong

#### 1. 实验目的

编写一个使用UNIX系统调用的程序来在两个进程之间“ping-pong”一个字节，请使用两个管道，每个方向一个。父进程应该向子进程发送一个字节;子进程应该打印“\<pid\>: received ping”，其中\<pid\>是进程ID，并在管道中写入字节发送给父进程，然后退出;父级应该从读取从子进程而来的字节，打印“\<pid\>: received pong”，然后退出。用户态源代码在文件``user/pingpong.c``中。

#### 2. 实验内容

在main函数中，用``fork()``创建一个子进程。创建两个管道。创建管道的具体操作为

```cpp
int pd[2];
pipe(fd);
```

在两个进程中对两个管道进行读写。并输出ping/pong，发送pong/ping.可以用``getpid()``获取当前进程的pid。

更改``Makefile``中的``UPROGS``，加入``$U/_pingpong\``

#### 3. 遇到的问题及解决方法

1.  不熟悉UNIX管道。不熟悉管道的相关操作，包括声明管道、写入或读取管道信息、关闭管道端等等。通过查阅相关资料了解了管道的基本使用方式。
2.  空闲管道端的关闭。起初，我没有关闭空闲管道端，造成了一些困扰。在课上老师反复强调进程是分配资源的单位。所以每个进程都要对管道端进行操作。
3.  进程同步互斥问题。起初，我在程序中规定控制台输出在进程的末尾。结果输出的是乱码。原因是在这种情况下，控制台是临界资源，只能互斥访问，而程序中没有进行同步互斥。解决方法有三种：
    1.  使用Peterson算法。
    2.  使用xv6自带的spinlock。
    3.  通过限制输出顺序消除同步互斥，即一旦接收到数据立刻输出，类似Tomasulo算法。最终我采用的是这种方法。

#### 4. 实验心得

这个实验虽小，但是收获颇丰。首先，我练习使用了``fork()``函数；其次，我熟悉了UNIX的管道机制；更重要的是，我在实践中首次遇到同步互斥问题，并想到Peterson算法，将理论运用到实际中。

#### 5. 实验截图

![pingpong截图](E:/大二下/操作系统/课设文档/src/Lab01/pingpong.bmp "pingpong实验截图")

## primes

#### 1. 实验目的

使用管道编写``prime sieve``的并发版本. 具体原理图如下:
![prime原理图](E:/大二下/操作系统/课设文档/src/Lab01/prime-prin.bmp "prime sieve")
图中每一个矩形框是一个进程，进程间通过管道传输数据，输入的数据是从2开始的连续自然数组成的数组。每个管道将数组第一个元素作为本管道输出的素数。并按顺序检验数组后续数字是否能被该素数整除。若不能，则将其写入另一数组中。完成该流程后，将此数组传给下一管道。源程序放在``/user/primes.c``。

#### 2. 实验内容

**通过编写一个递归函数实现不断向前推进的过程**，参数是左邻居进程创建的管道。由于xv6被分配的资源受限，这里只传到35.

1.  在main函数中创建初始管道，输入从2到35。
2.  在main函数中创建一个进程。若为子进程，则调用递归函数；若为父进程，则关闭管道端，等待子进程退出后退出。
3.  递归函数中，关闭传入管道写端的写端；创建一个管道，关闭读端。从父管道都第一个数，这是这个进程获取的素数，将其打印。接着依次读出管道内其他数。若不能被该进程获取的素数整除，则写入本进程创建的管道中。接着创建子进程，进入子进程（递归）。子进程结束后，父进程退出。
4.  更改``Makefile``中的``UPROGS``，加入``$U/_primes\``

#### 3. 遇到的问题及解决方法

一开始我没有规定父进程一定要等子进程退出后退出。出现父进程在子进程运行时退出的情况，导致操作系统继续接收用户输入。出现如下情况：

```bash
$
prime 2
$prime 3
prime 5
...
```

在规定父进程只有在子进程退出后才能退出后，该问题消失。由于本进程申请了多个子进程，容易发生资源不够的情况。所以不用的管道端一定要关闭，否则panic。

#### 4. 实验心得

本实验了解了``primes sieve``的原理并简单实现了``primes sieve``，对并行编程和递归思想有了全新的认识。多进程的递归可以充分发挥多核CPU的优势，提高递归性能。在实现"primes sieve" 算法时，算法的效率受到空间复杂度的影响。虽然该算法的时间复杂度较低，但它需要使用一个长度为n的数组来存储中间结果，因此空间复杂度为O(n)。对于较大的输入范围，可能需要考虑优化空间复杂度，例如使用位图来代替普通数组来节省空间。

#### 5. 实验截图

![primes截图](E:/大二下/操作系统/课设文档/src/Lab01/primes.bmp "primes实验截图")

## find

#### 1. 实验目的

编写一个简化版本的UNIX的``find``程序：查找目录树中具有特定名称的所有文件。源程序放在``/user/find.c``。

#### 2. 实验内容

参考``user/ls.c``中的内容，编写一个递归函数``void find(char *path, const char *filename)``实现对目录的递归查找。基本步骤是：

1. 声明变量，存放包括``open()``返回值的整型变量，路径结构体``struct dirent de``、分析结构体``struct stat st``等。
2. 打开管道。
3. 检查传入参数的正确性，包括是否能打开、``find()``的第一个参数是否是目录、是否可以分析、目录是否超长等等。
4. 排除``.``和``..``，若是文件，与目标文件名做``strcmp``，若一致则输出完整目录；若是目录，则递归查找。
5. 关闭管道。
6. 更改``Makefile``中的``UPROGS``，加入``$U/_find\``

#### 3. 遇到的问题及解决方法

在做实验之前没有注意到UNIX的ls命令会把``.``和``..``包括在内，所以一开始没有排除这两个目录，造成了一些困扰；搜索时没有用绝对目录，也报了一些错；一开始使用了strcpy，操作系统提示无此函数，后来改为用``memmove()``、``memset()``和``memcpy()``系列函数，解决了这些问题。

#### 4. 实验心得

之前在Windows下写过类似功能的函数，这次在实验中写了相似功能的实验，进一步熟悉了UNIX，并发现了WIndows和UNIX的一些不同，比如Windows使用句柄，而UNIX多用管道。另外，我还了解了一下echo。

#### 5. 实验截图

![find](E:/大二下/操作系统/课设文档/src/Lab01/find.bmp "find实验截图")


## xargs

#### 1. 实验目的

编写一个简化版UNIX的``xargs``程序：它从标准输入中按行读取，并且为每一行执行一个命令，将行作为参数提供给命令。源程序在``user/xargs.c``

#### 2. 实验内容

1. 获取前一命令的标准化输出：使用``read()``函数获取。``read(0, buf, MSGSIZE)``
2. 获取自己的命令行参数：通过带参main函数读取。定义大小为``MAXARG``大小的``char*``数组``xargv``，从1复制``argv``到``xargv``。
3. 如何使用exec执行命令： 读取buf中内容（以``\n``为终止符），并创建子进程。子进程先将操作放入``xargv``，再整理参数，放入``xargv``。最后调用``exec(xargv[0],xargv)``执行操作即可，子进程退出。父进程指向buf下一个字符并等待子进程退出。
4. 更改``Makefile``中的``UPROGS``，加入``$U/_xargs\``

#### 3. 遇到的问题及解决方法

由于对bash命令的了解有限，我不太了解xargs和grep是什么意思，所以对于实验要求也一知半解，在查阅资料并观看视频后，我了解到xargs的是“extended augrents”。它与管道符连用，用管道左侧的输出作为管道右侧的命令行参数。例如可以

```bash
echo 10 | xargs sleep # 相当于sleep 10
```

表示``sleep 10``。在阅读实验给出的提示后，我使用exec完成了执行。

#### 4. 实验心得

本实验让我了解了UNIX下的xargs命令，增进了对bash命令的了解。通过实现xargs，我更深入地理解了操作系统的工作原理，学会了如何在xv6中进行进程管理和管道通信。这让我对UNIX操作系统和相关的命令行工具有了更深入的认识。同时，这也锻炼了我的C语言编程能力和问题解决能力。我也明白了bash中的某些指令不需要进入内核态，而只需要调用系统调用即可完成，本质上是一个用户态的命令行程序。

#### 5. 实验截图

![xargs](E:/大二下/操作系统/课设文档/src/Lab01/xargs.bmp "xargs实验截图")

## make grade截图

![grade-lab01](E:/大二下/操作系统/课设文档/src/Lab01/grade-lab01.bmp "lab 01 grade")

## 代码位置

`user/sleep.c`

`user/pingpong.c`

`user/find.c`

`user/primes.c`

`user/xargs.c`

`Makefile col:196~200`

# Lab02 Sys call

## System call tracing

#### 1. 实验目的

在本作业中，一个系统调用跟踪功能将被实现。实验人员将创建一个新的`trace`系统调用来控制跟踪。它应该有一个参数，这个参数是一个整数“掩码”（mask），它的比特位指定要跟踪的系统调用。例如，要跟踪`fork`系统调用，程序调用`trace(1 << SYS_fork)`，其中`SYS_fork`是***kernel/syscall.h***中的系统调用编号。如果在掩码中设置了系统调用的编号，则必须修改xv6内核，以便在每个系统调用即将返回时打印出一行。该行应该包含进程id、系统调用的名称和返回值；不需要打印系统调用参数。`trace`系统调用应启用对调用它的进程及其随后派生的任何子进程的跟踪，但不应影响其他进程。

#### 2. 实验内容

1. 阅读`user/trace.c`中的内容。发现它首先调用`trace(int)`，再将`argv`除了前两个参数外的其他参数写入新的`nargv`中，再调用`exec`函数（util-xargs中也有）从而执行指令。
2. 在`kernel/proc.h`文件的`proc`结构体中加入一个新的字段`trace_mask`用于保存`trace`的参数。并在`sys_trace()`的实现中实现参数的保存，具体方法是调用`argint(int, (int *))`函数。
3. 根据题目提示，由于`struct proc`中增加了一个新的变量,当`fork`的时候我们也需要将这个变量传递到子进程中。具体做法是访问子进程的PCB，将相应字段拷贝过去。
4. 接下来考虑如何实现系统调用`trace`了，具体需要更改`kernel/sysproc.c syscall()`。需要注意的是，`trace_mask`是比特掩码，需要用位运算检查有哪些需要追踪。更改后的` syscall()`具体步骤是：
   1. 获取当前进程的PCB，`struct proc *p = myproc()`
   2. 获取该进程的系统调用编号，在`p->trapframe->a7`字段（寄存器）中，类型为`int`。
   3. 执行系统调用，将结果保存在`p->trapframe->a0`中。
   4. 通过`(1<<num) & p->trace_mask`检查是否匹配。
5. 在`kernel/syscall.c`中注册`sys_trace`。具体有三步：
   1. `extern uint64 sys_trace(void)`
   2. 注册函数：在`static uint64 (*syscalls[])(void)`中加入`[SYS_trace]   sys_trace,`
   3. 注册系统调用名：在`static char *syscalls_name[]`中加入`[SYS_trace]   "trace",`
6. 在***Makefile***的**UPROGS**中添加`$U/_trace`
7. 在***user/user.h***中加入`int trace(int);`
8. 在***user/usys.pl***中加入`entry("sysinfo");`

#### 3. 遇到的问题及解决方法

第一次在内核态下编程，对系统调用的分层架构和实现原理不太理解，导致不知道怎样注册系统调用，怎样获取进程控制块等操作。在阅读内核态其他函数的源码后，我对这些操作有了一定的认知。在编写syscall核心代码时，由于对位运算不太了解，我在列出系统调用时花费了一些时间。另外，由于需要获取进程的系统调用，我还去了解了一下对应寄存器的作用。

#### 4. 实验心得

lab01在用户态下通过系统调用实现内核态的操作。而lab02开始就是从内核态编写一个新的系统调用。我学习到了xv6下实现系统调用的全流程操作，包括注册、编写、调用。阅读了表驱动的系统调用注册表，包括名称、函数指针等。

#### 5. 实验截图

![trace](E:/大二下/操作系统/课设文档/src/Lab02/trace.bmp "trace实验截图")

## Sys info

#### 1. 实验目的

在这个作业中，您将添加一个系统调用`sysinfo`，它收集有关正在运行的系统的信息。系统调用采用一个参数：一个指向`struct sysinfo`的指针（参见***kernel/sysinfo.h***）。内核应该填写这个结构的字段：`freemem`字段应该设置为空闲内存的字节数，`nproc`字段应该设置为`state`字段不为`UNUSED`的进程数。我们提供了一个测试程序`sysinfotest`；如果输出“**sysinfotest: OK**”则通过。

#### 2. 实验内容

1. 在`kernel/kalloc.c`中添加一个函数`getFreeMem(void)`用于获取空闲内存量。由于内存是使用链表进行管理的，因此遍历`kmem`中的空闲链表就能够获取所有空闲内存，函数内是遍历链表的基本操作。
2. 在`kernel/proc.c`中添加一个函数`procnum(uint64 *dst)`获取当前系统进程数。具体方法为遍历`proc`数组，检查正在使用的PCB以获取进程数。
3. 实现`sys_sysinfo`，将数据写入结构体并传递到用户空间。
4. 注册该系统调用（同trace）
5. 注册用户态`sysinfotest`测试

#### 3. 遇到的问题及解决方法

本实验有三个板块：一、获取空闲块数；二、获取进程数；三、内核态向用户态传输数据。而这些都需要通过查阅xv6内核态源码获得。这需要理解其代码逻辑与对应的数据结构，有些耗时。

#### 4. 实验心得

在这个实验中，我学习了如何添加一个系统调用`sysinfo`，它可以收集有关系统的一些信息，比如空闲内存和进程数。这个实验让我对xv6的内存管理和进程管理有了更深入的了解，也锻炼了我的代码阅读和编写能力。通过这个实验，我实现了系统调用函数`sys_sysinfo`，它接受一个指向用户空间的结构体指针作为参数。这个结构体定义在`kernel/sysinfo.h`中，包含两个字段：`freemem`和`nproc`。我学习到可以使用`copyout()`函数将数据从内核空间复制到用户空间，从而实现内核态向用户态的数据传输。从本质上理解了用户态和内核态的不同——地址空间不同。只要可以通过指针写，就可以访问。

#### 5. 实验截图

## make grade 截图

![grade-lab02](E:/大二下/操作系统/课设文档/src/Lab02/grade-lab02.bmp "lab02成绩")

## 代码位置

`kernel/proc.h` col: 109

`kernel/sysproc.c` col: 101, 117

`kernel/proc.c` col: 274

`kernel/syscall.c` col: 158, 159, 162

`kernel/kalloc.c` col: 85

`kernel/proc.c` col: 664

# Lab03 Page table

## Speed up system calls

### 1. 实验目的

创建每个进程时，在 USYSCALL（一个 VA 定义的 在`memlayout.h`中）。在此页的开头，存储一个`结构 usyscall`（也在`memlayout.h`中定义），并将其初始化以存储 当前进程的 PID。对于这个实验室，`ugetpid（）` 已经 在用户空间端提供，并将自动使用 USYSCALL 映射。 如果`pgtbltest`中的`ugetpid`测试通过，实验完成。

### 2. 实验内容

1. 在`krenel/proc.c`中的`proc_pagetable(struct proc *p)`声明一个共享内存。映射`USYSCALL`，授权用户态与读，具体做法是`PTE_R | PTE_U`。并对内存初始化。
2. 如果第一步失败则解除与`USYSCALL`和`TRAMPOLINE`的映射关系，并释放内存。
3. 在`krenel/proc.c`中的`allocproc(void)`函数中分配一个`USYSCALL`页。
4. 在`krenel/proc.c`中的`freeproc(void)`函数中释放该页。
5. 在在`krenel/proc.c`中的`proc_freepagetable(void)`函数中解除与物理内存的映射关系。

### 3. 遇到的问题及解决方法

由于对用户态和内核态的区别和联系不太了解，不清楚如何添加这块共享内存，也不清楚如何设置共享内存的权限设置。另外，对内核态如何释放内存、如何解除映射关系花了一些时间阅读源码。

### 4. 实验心得

通过这个实验，我进一步深入了解了操作系统的内核编程和页表管理，了解了提高系统运行性能 的一种方法是在用户态和内核态之间建立只读的共享内存。我学会了在内核中为进程创建共享内存，设置页表映射关系和权限，以及在释放资源时解除映射关系并释放内存。同时，我对于调试和测试内核代码有了更多的经验。这让我对操作系统的内部工作原理有了更深入的理解，对于我未来的系统编程和操作系统研究有了更好的基础。

### 5. 实验截图

![getpid](E:/大二下/操作系统/课设文档/src/Lab03/getpid.bmp "getpid实验截图")

## Print a page table

### 1. 实验目的

定义一个名为 `vmprint（）` 的函数。 它应该有 一个`pagetable_t`参数，并打印该页表 采用下述格式。 在`exec.c `的`exec`函数中，在`return argc` 之前插入 `if（p->pid==1） vmprint（p->pagetable）`  ， 以打印第一个进程的页表。 如果通过 `Make Grade` 的 `PTE printout`测试，则实验完成。

### 2. 实验内容

1.  在`exec.c `的`exec`函数中，在`return argc` 之前插入 `if（p->pid==1） vmprint（p->pagetable）`  ， 以打印第一个进程的页表。
2.  早`kernel/vm.c`中声明一个`vmprint`函数，该函数是一个递归函数。
3.  阅读参考`freewallk`函数部分代码，可知一页有512个页表项。遍历它们，如果是页表地址则递归调用。利用了DFS的思想。
4.  限制递归深度为2（系统资源有限，页表深度也不大）。
5.  
6.  用`%p`打印地址。

### 3. 遇到的问题及解决方法

虽然直到采用DFS的思想遍历页表，但一页有多少页表项一开始不太清楚。后来根据实验提示查阅了`freewalk`函数后获取了页表项。第二个问题是一开始在printf的格式化字符串中没有用%p打印地址，造成了一些困扰。

### 4. 实验心得

通过解决上述问题，我成功地实现了`vmprint`函数，并且能够在`exec`函数中打印第一个进程的页表。这个实验让我更深入地理解了页表的结构和页表项的数量，以及如何使用递归来遍历页表。在完成这个试验后，我对虚拟内存与多级页表有了更深入的理解。

### 5. 实验截图

![print](E:/大二下/操作系统/课设文档/src/Lab03/print.bmp "print pgtbl实验截图")

## Detecting which pages have been accessed

### 1. 实验目的

本实验中，一个名为 `pgaccess()`的系统调用函数将被实现，它返回已被访问的页。该系统调用接收三个参数。首先，它需要 要检查的第一个用户页面的起始虚拟地址。其次，它需要 要检查的页数。最后，它将用户地址带到缓冲区进行存储 结果进入位掩码（一种每页使用一位的数据结构，其中 第一页对应于最低有效位）。

### 2. 实验内容

1. 在`kernel/sysproc.c`中补全`sys_pgaccess()`函数：
2. 在`sys_pgaccess()`中，用`argaddr()`和`argint()`接收参数（内核态下接收参数与用户态不同）。接收页表个数、掩码、和入口地址（虚拟）。
3. 在`sys_pgaccess()`中，建立一个中间变量保存那些页表被访问过。这里用的是一个int变量。
4. 设置页表个数上限，这里设置了32.
5. 在`kernel/riscv.h`中定义`PTE_A`，值为`1 << 6`.
6. 在`pgaccess()`中，从入口地址开始遍历，调用`vmpgaccess()`（在`kernel/vm.c`中实现）。并按位与到中间变量对应位。
7. 在`kernel/vm.c`中，定义`vmpgaccess(pagetable_t pagetable, uint64 va)`。在该函数中判断一个页是否被访问。具体方法为调用walk函数返回页表物理入口，再将`PTE_A`置为0，返回1.若入口为0，就返回0.

### 3. 遇到的问题及解决方法

不了解虚拟地址、物理地址的读取方法与转换方法；对多级页表的认识只停留于纸上，面对真正的操作系统无法调用系统函数实现逻辑。主要是不了解内核态下文件的意义以及其中函数实现的功能，加之语言障碍对我产生了非常大的困难。挣扎一番后决定看实验提示并结合多级页表的图思考。一条条看过去后逐渐有了思路。

### 4. 实验心得

初看这个实验是有一种无从下手的感觉，内核态仍然是黑箱子的存在。在看实验提示后对标志位有了新的认识。在解决问题的过程中，我学会了如何在内核态下接收参数，如何进行虚拟地址和物理地址的转换，以及如何操作多级页表来判断页是否被访问过。同时，我也更加熟悉了操作系统的内核编程环境和文件结构。

### 5. 实验截图

![detect](E:/大二下/操作系统/课设文档/src/Lab03/detect.bmp "detect实验截图")

## Make grade 截图

![grade-lab03](E:/大二下/操作系统/课设文档/src/Lab03/grade-lab03.bmp "lab03 grade")

## 代码位置

**speed up system calls**

`kernel/proc.c` col: 130，159，227，211

**print a page table**

`kernel/exec.c` col: 121

`kernel/vm.c` col: 444

**detecting which pages have been accessed**

`kernel/sysproc.c`  col: 81

`kernel/vm.c` col: 466

`kernel/riscv.h` col: 346

# Lab04 Traps

## RISC-V assembly

### 实验目的

回答问题。

### 实验内容

编译一个程序，回答问题。

### 遇到的问题

无

### 实验心得

通过观察汇编代码了解了高级语言向汇编的转换，更进一步接近底层了。

## Back trace

### 实验目的

在`kernel/printf.c`中实现一个函数`backtrace()`。在`sys_sleep`中插入这个函数，然后执行`bttest`以调用`sys_sleep`。输出应该如下：

```bash
backtrace:
0x0000000080002cda
0x0000000080002bb6
0x0000000080002898
```

这个函数实际就是实现曾经调用函数地址的回溯。

### 实验内容

1. 在`kernel/defs.h`中加入函数声明，并在`kernel/printf.c`中加入`void backtrace()`函数。
2. 在`kernel/riscv.h`中加入实验给出的`r_fp()`函数用于读取寄存器fp中的值。
3. 在`sys_sleep()`中调用`backtrace()`
4. 实现backtrace：
   1. 将fp传入`PGROUNDUP`和`PGROUNDDOWN`并做差。当差等于页表大小时做while循环
   2. 在循环内，返回地址保存在-8偏移的位置
   3. 前一个页帧指针保存在-16偏移的位置

### 遇到的问题

不太理解操作系统栈空间和栈分布。后来经过查阅资料与观看视频后对“函数调用栈”有了一定的认知：它是由高地址向低地址增长，且大小为一页的栈。另外，栈指针保存在sp寄存器中，且初始在高地址。另外，一开始我担心指针越界的问题，后来了解到用`PGROUNDUP`和`PGROUNDDOWN`可以获取栈帧所在页的页地址的范围。

### 实验心得

在完成这个实验时，我遇到了一些挑战，尤其是在理解操作系统栈空间和栈分布方面。最初，我对函数调用栈的工作原理不太了解，包括栈指针的位置以及栈帧的保存方式。这导致我在实现`backtrace()`函数时感到困惑。通过查阅资料和观看相关视频，我逐渐理解了其工作原理。每个函数调用都会在栈上创建一个栈帧，用于保存函数的返回地址、参数和局部变量等信息。理解了这些概念后，我开始着手实现`backtrace()`函数。做完这个实验我对调试中的追踪功能产生了深深的敬意，挺难实现的。

### 实验截图

![backtrace](E:/大二下/操作系统/课设文档/src/Lab04/backtrace.bmp "backtrace截图")

## Alarm

### 实验目的



### 实验内容

1. 在Makefile中将`alarmtest.c`添加至UPROGS；将sigalarm和sigreturn声明为系统调用（见lab03）；sigalarm接收参数（见lab03）；在盘`kernel/proc.h`中，修改proc结构体，增加`int ticks`（系统调用指定，表示经过几个时钟周期调用sigalarm）、`int ticks_cnt`、`tick_epc`（表示目前累积了多少时钟周期）、`uint64 handler`（中断函数地址）字段、`int handler_exec`（存储handler函数是否在被执行，防止二次进入）以及32个通用寄存器（用于保存和恢复现场），并在`allcoproc`中初始化。
2. 在`kernel/trap.c`中添加store函数，存储32个通用寄存器
3. 在`kernel/sysproc.c`中添加restore函数，恢复32个通用寄存器
4. 在`kernel/trap.c`的`usertrap(void)`中，在`if(which_dev == 2)`的条件下，记录tick次数。若`ticks_cnt > ticks`，则保存现场，同时设置此时handler函数已被调用；用sigreturn恢复现场。

### 遇到的问题及解决方法

找不到函数指针在trapframe哪个寄存器里；一开始有点困惑该保留哪些寄存器，后来发现要全部保存......在测试时报错，说多次执行了handler，本来想通过同步互斥解决，但想了想感觉思路错了，最后在proc结构体里又加了一个字段……

### 实验心得

在课上对这个trap机制不是很理解，通过实验理解保存的现场在PCB中；同时，完全掌握了内核态获取参数的方法。感觉在保存和恢复的时候可以用memcopy函数，但是没想那么多直接写了。总的来说，这个实验是实现两个系统调用，并修改usertrap。还是非常有收获的。

### 实验截图

![alarm](E:/大二下/操作系统/课设文档/src/Lab04/alarm.bmp)

## Make grade截图

![grade-lab04](E:/大二下/操作系统/课设文档/src/Lab04/grade-lab04.bmp)

## 代码位置

**backtrace**

`kernel/printf.c` col: 137

`kernel/riscv.h` col: 326



## 附：gdb调试xv6代码

### 调试步骤：

1. 在一个终端输入`make qemu-gdb`，可以指定CPU个数执行，如`make CPUS=1 qemu-gdb`，在make qemu后会提示去另一个终端启动gdb，并给出此终端的端口号。

   ![gdb-port](E:/大二下/操作系统/课设文档/src/Lab04/gdb-port.bmp)

2. 新建一个终端，输入`gdb-multiarch kernel/kernel`，输入以下代码，注意端口号输自己的。

   ```bash
   (gdb) set architecture riscv:rv64
   (gdb) target remote localhost:26000
   ```

   

3. 加载文件。如果调试的是用户态文件，则需要加一个`user/`，如：

   ```
   (gdb) file user/mkdir
   ```

   

4. 打断点，用`b xxx`命令。一般会打在main开头，`b main`；或打在某一行，如`b trace.c:20`表示在`mkdir.c`的第20行打断点。

5. 继续执行，用`c`命令。

6. 触发断点后，按步执行即可，以下给出gdb常用指令。

   | 指令名       | 指令作用                        |
   | ------------ | ------------------------------- |
   | b            | 打断点                          |
   | c            | 继续执行                        |
   | layout split | 显示源代码和汇编代码            |
   | ni           | 单步汇编执行，不进函数          |
   | si           | 单步汇编执行，进入函数          |
   | n            | 单步C语言执行，对应VS中“逐过程” |
   | s            | 单步C语言执行，对应VS中“逐语句” |
   | p $a0        | 打印a0寄存器中的值              |
   | p/x 1536     | 以16进制格式打印1536            |
   | i r a0       | a0寄存器信息                    |
   | x/i 0x630    | 查看0x630地址处的指令           |

### 调试效果

![gdb](E:/大二下/操作系统/课设文档/src/Lab04/gdb.bmp)

# Lab05 Copy-on-Write Fork for xv6

## Implement copy-on write

### 实验目的

#### 问题

xv6中的`fork()`系统调用将父进程的所有用户空间内存复制到子进程中。如果父进程较大，则复制可能需要很长时间。更糟糕的是，这项工作经常造成大量浪费；例如，子进程中的`fork()`后跟`exec()`将导致子进程丢弃复制的内存，而其中的大部分可能都从未使用过。另一方面，如果父子进程都使用一个页面，并且其中一个或两个对该页面有写操作，则确实需要复制。

#### 解决方案

copy-on-write (COW) fork()的目标是推迟到子进程实际需要物理内存拷贝时再进行分配和复制物理内存页面。

COW fork()只为子进程创建一个页表，用户内存的PTE指向父进程的物理页。COW fork()将父进程和子进程中的所有用户PTE标记为不可写。当任一进程试图写入其中一个COW页时，CPU将强制产生页面错误。内核页面错误处理程序检测到这种情况将为出错进程分配一页物理内存，将原始页复制到新页中，并修改出错进程中的相关PTE指向新的页面，将PTE标记为可写。当页面错误处理程序返回时，用户进程将能够写入其页面副本。

COW fork()将使得释放用户内存的物理页面变得更加棘手。给定的物理页可能会被多个进程的页表引用，并且只有在最后一个引用消失时才应该被释放。

### 实验内容

1. 在`kernel/riscv.h`中定义`PTE_F`标记一个页面是否为COW Fork的标志位。
2. 在`kernel/kalloc.c`中进行修改：
   1. 定义引用计数的全局变量`ref`。有一个自旋锁和一个引用计数的数组，并在`kint()`中初始化该自旋锁。
   2. 修改`kalloc`和`kfree`函数，在`kalloc`中初始化内存引用计数为1，在`kfree`函数中对内存引用计数减1，如果引用计数为0时才真正删除。
   3. 添加`cowpage(pagetable_t, uint64)`，功能为判断一个页面是否为COW页面，具体步骤为：获取pte，检查`PTE_V`是否为1（是否合法pte），返回`PTE_F`的值。
   4. 添加`cowalloc(pagetable, uint64)`，功能为cow分配器。具体步骤为：获取对应物理地址；获取对应PTE；获取引用计数，若引用计数为1则直接修改对应PTE，若引用计数大于1则分配页面并拷贝旧页面内容；清除PTE_V，防止remap；为新页面添加映射；将原来的物理内存引用计数减一。
   5. 添加`krefcnt(void *pa)`，功能为获取内存的引用计数`ref.cnt[(uint64)pa / PGSIZE]`。
   6. 添加`kaddrefcnt(void *pa)`，功能为添加内存的引用计数。操作为上锁后对ref中引用计数变量+1
   7. 将这四个函数在`kernel/defs.h`中声明。
   8. 修改`freerange`函数，注意在free前对引用计数置一后再kfree。
3. 修改uvmcopy，不为子进程分配内存，而是使父进程共享内存。但禁用`PTE_W`，同时标记`PTE_F`，最后调用`kaddrefcnt`增加引用计数。
4. 修改usertrap，处理错误页面。
5. 在copyout中处理相同的情况，如果是COW页面，需要更换`pa0`指向的物理地址。

### 遇到的问题和解决方法

对写时拷贝不太理解，通过实验的解释了解了写时拷贝的原理，并了解了其逻辑——引用计数。通过对页面的引用计数，操作系统可以判断那些页可以被回收。在写的过程中，由于没有把引用计数置一，所有在释放时把它减到了负数。另外，思考引用计数数组如何索引，确保正确性的操作花了一些时间。写时拷贝涉及fork的全流程，所以在写的时候有很多顾虑，后来在实验指导的提示下一步步解决了问题。

### 实验心得

这个实验让我对写时拷贝（COW）有了更深入的理解。我学会了如何通过引用计数来推迟内存复制，从而提高系统的性能和效率。同时，我也学会了在实现写时拷贝时，要注意引用计数数组的正确索引和页面错误处理等细节。这个实验让我明白操作系统的优化是复杂的，对程序员来说明白要干什么比写代码更重要。通过这个实验，我不仅提高了对写时拷贝的理解，还加深了对操作系统内核编程的认识。这个实验对我来说是一个很好的学习机会，让我更加深入地了解了操作系统的工作原理和优化技术。

### 实验截图

![cow](E:/大二下/操作系统/课设文档/src/Lab05/cow.bmp "cow实验截图")

## Make grade截图

![grade-lab05](E:/大二下/操作系统/课设文档/src/Lab05/grade-lab05.bmp "lab05 grade")

## 代码位置

`kernel/riscv.h` col: 346

`kernel/kalloc.c` col: 17, 36,  41，129

`kernel/vm.c` col: 332

`kernel/trap.c` col: 37

`kernel/vm,c` col: 380

# Lab06 Multitreading

## Uthread: switching between threads

### 实验目的

本实验是在给定的代码基础上实现用户级线程切换。因为是用户级线程，不需要设计用户栈和内核栈，用户页表和内核页表等等切换。

### 实验内容

1. 在`user/uthread.c`中定义存储上下文的结构体context_t
2. 在`user/uthread.c`中修改`thread`结构体，添加`context`字段。
3. 模仿`kernel/swtch.S`，在`kernel/uthread_switch.S`中写入代码用于上下文切换
4. 在`user/uthread.c`中修改`thread_schedule()`，添加线程切换语句
5. 在`user/uthread.c`中，在`thread_create`中对`thread`结构体做一些初始化设定，主要是`ra`返回地址和`sp`栈指针。

### 遇到的问题和解决方法

对上下文切换具体原理不是很了解，参考***switch.S***后对代码的写法有了了解。

### 实验心得

实现了用户态线程，可以感觉到开销比内核态线程小，因为不用费劲的维护例如`trapframe`的数据。

### 实验截图

![uthread](E:/大二下/操作系统/课设文档/src/Lab06/uthread.bmp)

## Using threads

### 实验目的

在探索使用哈希表的线程和锁的并行编程中，解决多线程读写数据时散列桶数据丢失的问题。

### 实验内容

1. 在`ph.c`中为每个散列桶定义一个锁，将五个锁放在一个数组中，并进行初始化。
2. 在`put()`中对`insert()`上锁。

### 遇到的问题和解决方法

无

### 实验心得

将同步互斥的知识运用到实践中。

### 实验截图

![ph_safe](E:/大二下/操作系统/课设文档/src/Lab06/ph_safe.bmp)

## Barrier

### 实验目的

实现一个屏障：应用程序中的一个点，所有参与的线程在此点上必须等待，直到所有其他参与线程也达到该点。在实验中，使用pthread条件变量。这是一种序列协调技术，类似于xv6的`sleep`和`wakeup`。

### 实验内容

完成`notxv6/barrier.c`中`barrier(void)`函数。具体操作为：

1. 申请锁
2. 调用barrier的线程+1
3. 判断是否所有线程调用barrier
   1. 是：轮次+1，线程数清零
   2. 否：等待未到达的线程
4. 释放锁。

### 遇到的问题和解决方法

与信号量隔离不同，这里用的是一个函数进行隔离的实现。学到了用函数隔离的方法。一开始以为允许线程在两个轮次执行，但是总是失败。后来参考别人的思路了解到应当等所有线程都调用barrier后再开启新的轮次。

### 实验心得

了解了UNIX下`barrier(void)`的实现。

### 实验截图

![uthread](E:/大二下/操作系统/课设文档/src/Lab06/barrier.bmp)

## Make grade截图

![uthread](E:/大二下/操作系统/课设文档/src/Lab06/grade-lab06.bmp)



## 代码位置

**Uthread: switching between threadsd**

`user/uthread.c` col: 14, 33

`user/uthread_switch.S` col: 14

`user/thread.c` col: 89, 104

**Using threads**

`notxv6/ph.c` col: 58

**Barrier**

`notxv6/barrier.c` col: 33

# Lab07 networking

## network driver

### 实验目的

在***kernel/e1000.c***中完成`e1000_transmit()`和`e1000_recv()`，以便驱动程序可以发送和接收数据包。，以实现用e1000设备进行网络通信。

### 实验内容

1. 完成`e1000_transmit()`
   1. 上锁
   2. 获取tx环索引，若越界则报错
   3. 填充描述符
   4. 更新环位置
   5. 解锁
2. 完成`e1000_recv()`
   1. 获取rx索引，若没有数据则向后移动一位。
   2. 判断包是否可用，即 E1000 是否真的写入数据包了
   3. 更新包缓冲区，即填充mbuf结构体，并推送网络栈
   4. 为刚才的mbuf分配kongjian
   5. 更新尾指针，更新为最后处理的环描述符的索引

### 遇到的问题和解决方法

对网络通信与网卡驱动相关知识不了解。解决方法是跟着hint做，并查阅csdn、博客园等网站。

### 实验心得

是一个生产者-消费者模型的应用，原因是数据的写入在一个循环索引上；且有头指针和尾指针，若两个指针指向同一索引则视为队列满

## Make grade 截图

![grade-lab07](E:/大二下/操作系统/课设文档/src/Lab07/grade-lab07.bmp)

## 代码位置

`kernel/e1000.c` col: 96，147

# Lab08 locks

## Memory allocator

### 实验目的

本实验完成的任务是为每个CPU都维护一个空闲列表，初始时将所有的空闲内存分配到某个CPU，此后各个CPU需要内存时，如果当前CPU的空闲列表上没有，则窃取其他CPU的。例如，所有的空闲内存初始分配到CPU0，当CPU1需要内存时就会窃取CPU0的，而使用完成后就挂在CPU1的空闲列表，此后CPU1再次需要内存时就可以从自己的空闲列表中取。

### 实验内容

1. 将`kmem`定义为一个数组，包含`NCPU`个元素，即每个CPU对应一个
2. 修改`kinit`，为所有锁初始化以“kmem”开头的名称，该函数只会被一个CPU调用，`freerange`调用`kfree`将所有空闲内存挂在该CPU的空闲列表上
3. 修改`kfree`，使用`cpuid()`和它返回的结果时必须关中断
4. 修改`kalloc`，使得在当前CPU的空闲列表没有可分配内存时窃取其他内存的

### 遇到的困难和解决方法

在kalloc和kfree中没有关中断，导致在`writebig`中出现`panic: balloc: out of blocks`。关中断的作用是使操作具有原子性，防止出现在释放时切换进程而可能导致的块不够的问题。通过阅读hint了解到需要添加关中断与开中断。

不太会用snprintf进行字符串格式化，通过阅读源码解决。

### 实验心得

通过这个实验，我学会了如何在多CPU环境下共享内存分配器，并了解了处理中断和原子操作的重要性。我理解了关中断的作用，以及如何确保操作的原子性以避免竞态条件和错误。在实验中，应该确保在分配和释放内存时，只操作当前CPU的空闲列表，并在需要时窃取其他CPU的内存。我对多个CPU之间的同步和共享问题有了更深入的理解。

### 实验截图

![kalloctest](E:/大二下/操作系统/课设文档/src/Lab08/kalloctest.bmp)

## Buffer cache

### 实验目的

如果多个进程密集地使用文件系统，它们可能会争夺`bcache.lock`，它保护`kernel/bio.c`中的磁盘块缓存。修改块缓存，以便在运行`bcachetest`时，buffer cache中所有锁的`acquire`循环迭代次数接近于零。修改`bget`和`brelse`，以便buffer cache中不同块的并发查找和释放不太可能在锁上发生冲突。

### 实验内容

1. 定义哈希桶结构，并在`bcache`中删除全局缓冲区链表，改为使用素数个散列桶
2. 在`binit`中:
   1. 初始化散列桶的锁
   2. 将所有散列桶的`head->prev`、`head->next`都指向自身表示为空
   3. 将所有的缓冲区挂载到`bucket[0]`桶上
3. 在`buf.h`中增加新字段`timestamp`。在原始方案中，每次`brelse`都将被释放的缓冲区挂载到链表头，禀明这个缓冲区最近刚刚被使用过，在`bget`中分配时从链表尾向前查找，这样符合条件的第一个就是最久未使用的。而在提示中建议使用时间戳作为LRU判定的法则，这样我们就无需在`brelse`中进行头插法更改结点位置
4. 更改`brelse`，不再获取全局锁
5. 更改`bget`，当没有找到指定的缓冲区时进行分配，分配方式是优先从当前列表遍历，找到一个没有引用且`timestamp`最小的缓冲区，如果没有就申请下一个桶的锁，并遍历该桶，找到后将该缓冲区从原来的桶移动到当前桶中，最多将所有桶都遍历完。在代码中要注意锁的释放
6. 最后将`bpin`和`bunpin`修改，获取对应哈希桶的锁，对对应哈希桶进行操作。

### 遇到的困难和解决方法

1. 哈希桶设置为13时出现`panic`，原因是xv6系统中块不足了。于是减少哈希桶到5，但一直不行。第二天再试突然行了，猜测`make clean`后再`make qemu`应该可以。但是在`make grade`中第一个测试失败了。将哈希桶设置为7后所有测试通过。
2. 在bget时发生了死锁。因为bget中重新分配可能要持有两个锁，如果桶a持有自己的锁，再申请桶b的锁，与此同时如果桶b持有自己的锁，再申请桶a的锁就会造成死锁。解决方法是加入检查语句：`if(!holding(&bcache.bucket[i].lock))`。为了避免频繁的跨桶获取缓冲区，规定优先从自身的桶中获取。

### 实验心得

通过这个实验，我学到了如何优化多个进程对块缓存的访问，以减少锁的竞争和冲突。我深入理解了哈希桶的概念以及如何在多CPU环境下进行锁的获取和释放，以确保并发访问的正确性和性能。在解决困难的过程中，我不仅提高了对操作系统内核编程和并发访问的理解，还锻炼了调试和优化的能力。这个实验让我更加深入地了解了操作系统内部的工作原理。

### 实验截图

![bcachetest](E:/大二下/操作系统/课设文档/src/Lab08/bcachetest.bmp)

## Make grade

![grade-lab08](E:/大二下/操作系统/课设文档/src/Lab08/grade-lab08.bmp)

## 代码位置

**Memory allocator**

`kernel/kalloc.c`: 24, 28, 56, 84

**Buffer cache**

`kernel/bio.c`: 26, 45, 

`kernel/buf.h`: 12, 61, 133, 152, 162

# Lab09 file system

## Large files

### 实验目的

本实验的目的是增加xv6文件的最大大小。由于目前一个xv6 inode包含12个“直接”块号和一个“间接”块号，“一级间接”块指一个最多可容纳256个块号的块，总共12+256=268个块，所以，xv6文件限制为`268*BSIZE`字节（在xv6中`BSIZE`为1024）。更改xv6文件系统代码，以支持每个inode中可包含256个一级间接块地址的“二级间接”块，每个一级间接块最多可以包含256个数据块地址。结果将是一个文件将能够包含多达65803个块，或256*256+256+11个块（11而不是12，因为我们将为二级间接块牺牲一个直接块号）。

### 实验内容

1. 在`fs.h`中添加宏定义：NDIRECT是直接块个数；NINDIRECT是一级块个数；NDINDIRECT是二级块个数。
2. 由于`NDIRECT`定义改变，其中一个直接块变为了二级间接块，需要修改inode结构体中`addrs`元素数量。
3. 修改`bmap`支持二级索引。
4. 修改`itrunc`释放所有块。

### 遇到的问题和解决方法

二级目录相较一级目录多了一次for循环，其他还好。就是需要追踪文件块被定义、访问和释放的全过程，有些繁琐。

### 实验心得

在真实的操作系统中实现了二级目录，很有意义。

### 实验截图

![bigfile](E:/大二下/操作系统/课设文档/src/Lab09/bigfile.bmp)

## Symbolic links

### 实验目的

添加符号链接，以实现类似快捷方式的功能。实现`symlink(char *target, char *path)`系统调用，该调用在引用由`target`命名的文件的路径处创建一个新的符号链接。有关更多信息，请参阅`symlink`手册页（注：执行`man symlink`）。要进行测试，请将`symlinktest`添加到***Makefile***并运行它。

### 实验内容

1. 配置系统调用的常规操作，如在`user/usys.pl`、`user/user.h`中添加一个条目，在`kernel/syscall.c`、`kernel/syscall.h`中添加相关内容
2. 添加提示中的相关定义，`T_SYMLINK`以及`O_NOFOLLOW`
3. 在`kernel/sysfile.c`中实现`sys_symlink`，这里需要注意的是`create`返回已加锁的inode，此外`iunlockput`既对inode解锁，还将其引用计数减1，计数为0时回收此inode
4. 修改`sys_open`支持打开符号链接

### 遇到的问题和解决方法

这个系统调用应该在`kernel/sysfile.c`中实现，而我在`kernel/sysproc.c`中实现时发现无法调用create函数，因为它被声明为静态。

程序中`begin_op()`和`end_op()`令我费解。进行搜索后得知这两句话既声明了对文件系统操作的开始和结束，也让操作系统确定了执行操作的时机——在完成所有更新之前，不执行任何一个操作。

### 实验心得

在课堂上对软连接的讲解十分有限，实验补足了我在这方面的认知。

### 实验截图

![symlink](E:/大二下/操作系统/课设文档/src/Lab09/symlink.bmp)

## Make grade

![grade-lab09](E:/大二下/操作系统/课设文档/src/Lab09/grade-lab09.bmp)

## 代码位置

**Large files**

`kernel/fs.h`: 27, 34

`kernel/fs.c`: 378, 438

**Symbolic links**

`kernel/sys_file.c`: 289, 525

# Lab10 mmap

## mmap

### 实验目的

本实验的主要目标是实现一个内存映射文件的功能，通过将文件映射到进程的地址空间中，从而在与文件交互时极大地减少磁盘操作的需求。内存映射文件是一种高效的文件访问方式，它允许程序直接访问文件在内存中的副本，而无需频繁地进行读取或写入磁盘操作，从而提高了文件访问的速度和效率。

### 实验内容

1. 配置系统调用`mmap`和`munmap`。
2. 在`kernel/proc.h`中定义VMA结构体，并添加到PCB中。
3. 在allocproc中将vma数组初始化为全0。
4. 根据提示2、3、4，将当前`p->sz`作为分配的虚拟起始地址，但不实际分配物理页面，实现`mmap`。此函数写在`sysfile.c`中就可以使用静态函数`argfd`同时解析文件描述符和`struct file`。
5. 根据提示5，此时访问对应的页面就会产生页面错误，需要在`usertrap`中进行处理，主要完成三项工作：分配物理页面，读取文件内容，添加映射关系。
   1. 在usertrap中，读取产生页面故障的虚拟地址，并判断是否位于有效区间。若在，尝试修复（调用自定义函数）。若不成功则kill进程；若不在，直接kill进程。
   2. 实现上述自定义函数`mmap_handler`。

6. 根据提示6实现`munmap`，且提示7中说明无需查看脏位就可写回。
7. 如果对惰性分配的页面调用了`uvmunmap`，或者子进程在fork中调用`uvmcopy`复制了父进程惰性分配的页面都会导致panic，因此需要修改`uvmunmap`和`uvmcopy`检查`PTE_V`后不再`panic`。
8. 根据提示8修改`exit`，将进程的已映射区域取消映射。
9. 根据提示9，修改`fork`，复制父进程的VMA并增加文件引用计数。

### 遇到的问题和解决方法

代码位置涉及多个文件，所有涉及跨文件的宏定义。结果是在编译时疯狂报错。解决方法是不断根据错误include，但似乎内核态头文件有不同的包含方式。在include后内核态头文件会报错，来回捣鼓了一会才搞定。

但上面的还是小问题。2021的实验没有lazy实验，所以为了实现“惰性地填写页表，以相应页错误”费了一番功夫。例如没有更改`uvmunmap`和`uvmcopy`检查`PTE_V`后不再`panic`导致出现panic的情况等等。

### 实验心得

之前很多实验都是实现操作系统的功能，而这个实验是对文件系统进行优化。造轮子相比优化轮子更难，因为既要让原有代码正确运行，又要实现性能的提高。对于xv6这样的教学用操作系统尚且困难，对Windows这种复杂的操作系统想必更难维护和优化。

## Make Grade

![grade-lab10](E:/大二下/操作系统/课设文档/src/Lab10/grade-lab10.bmp)

## 代码位置

`kernel/proc.h`: 87

`kernel/sysfile.c`: 488, 540

`kernel/trap.c`: 36, 132

`kernel/proc.c`: 366

